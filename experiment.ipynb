{
  "cells": [
    {
      "metadata": {
        "id": "459620ae91a86e79"
      },
      "cell_type": "markdown",
      "source": [
        "Where the actual experiment happens!"
      ],
      "id": "459620ae91a86e79"
    },
    {
      "cell_type": "code",
      "source": [
        "#import the repository\n",
        "!git clone https://github.com/steinburglar/REAN\n",
        "%cd REAN\n"
      ],
      "metadata": {
        "id": "6_8ONTWxEqom",
        "outputId": "0bd12f22-51c3-4fe0-8979-76e5caba60c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6_8ONTWxEqom",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'REAN'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 163 (delta 31), reused 148 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (163/163), 22.77 MiB | 19.83 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "[Errno 2] No such file or directory: 'yourrepo'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/REAN"
      ],
      "metadata": {
        "id": "llQrl2yMGas3",
        "outputId": "a0755f1b-5b8e-423e-c59a-fbc32f8b05cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "llQrl2yMGas3",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/REAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D_UUb20rFUZ1",
        "outputId": "c77889ac-d7db-4352-ad2f-1ed367ba357f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "D_UUb20rFUZ1",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "c2574f9b77488830"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 9,
      "source": [
        "\n",
        "#do necessary imports\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from rean.utils import make_run_dir, to_serializable\n",
        "from rean.data.Dataset import make_datasets\n",
        "from rean.models.CNN import PlainCNN\n",
        "from rean.models.P4 import P4CNN\n",
        "from rean.models.RelaxedP4 import RelaxedP4CNN\n",
        "from rean.train import train_full, evaluate\n",
        "from rean.plot import LossPlot\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "id": "c2574f9b77488830"
    },
    {
      "metadata": {
        "id": "505361abe207d126"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 22,
      "source": [
        "#some params that should be the same across all experimental runs\n",
        "\n",
        "group_order = 4\n",
        "hidden_dim = 20\n",
        "out_channels = hidden_dim\n",
        "classes = 10\n",
        "kernel_size = 3\n",
        "num_gconvs = 6\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.002\n",
        "gamma = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "id": "505361abe207d126"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_run_dir(model_name,\n",
        "                 noise_type,\n",
        "                 noise_params,\n",
        "                 learning_rate,\n",
        "\n",
        "                 ):\n",
        "    gamma = noise_params.get(\"gamma\")\n",
        "    std = noise_params.get(\"std\")\n",
        "    run_dir = Path(f\"/content/drive/MyDrive/runs/{model_name}_{noise_type}_std{std}_gamma{gamma}_lr{learning_rate}\")\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return run_dir"
      ],
      "metadata": {
        "id": "X_qdsU7pHPhr"
      },
      "id": "X_qdsU7pHPhr",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5f171478b923a65"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment Running Loop #"
      ],
      "id": "c5f171478b923a65"
    },
    {
      "metadata": {
        "id": "78e5ef70973b81ce",
        "outputId": "ad111356-6eb0-4dc7-b58a-a33c10c68569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25400 trainable parameters in P4CNN model\n",
            "Epoch 1/40, Train Loss: 1.3796, Val Loss: 0.7328, Val Acc: 74.32%\n",
            "Epoch 2/40, Train Loss: 0.5954, Val Loss: 0.4277, Val Acc: 86.90%\n",
            "Epoch 3/40, Train Loss: 0.3735, Val Loss: 0.4723, Val Acc: 87.27%\n",
            "Epoch 4/40, Train Loss: 0.2939, Val Loss: 0.2620, Val Acc: 91.80%\n",
            "Epoch 5/40, Train Loss: 0.2414, Val Loss: 0.2621, Val Acc: 92.14%\n",
            "Epoch 6/40, Train Loss: 0.2127, Val Loss: 0.1836, Val Acc: 94.49%\n",
            "Epoch 7/40, Train Loss: 0.2035, Val Loss: 0.1841, Val Acc: 94.41%\n",
            "Epoch 8/40, Train Loss: 0.1869, Val Loss: 0.3200, Val Acc: 90.21%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3060432412.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                             group_order=group_order)\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             run_data, best_model = train_full(model_name=model_name,\n\u001b[0m\u001b[1;32m     26\u001b[0m                           \u001b[0mtrain_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                           \u001b[0mval_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/REAN/rean/train.py\u001b[0m in \u001b[0;36mtrain_full\u001b[0;34m(model_name, num_epochs, train_ds, val_ds, device, batch_size, learning_rate, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/REAN/rean/train.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 21,
      "source": [
        "#test full loop\n",
        "models = ['P4CNN', 'RelaxedP4CNN']\n",
        "noise_types = ['none', 'iso', 'aniso']\n",
        "stds = [0.1, 0.2, 0.3]\n",
        "\n",
        "for model_name in models:\n",
        "    for noise_type in noise_types:\n",
        "        for i, std in enumerate(stds):\n",
        "            if noise_type == 'none':\n",
        "                if i != 0: #only run noiseless once\n",
        "                    continue\n",
        "                train_noise = None\n",
        "                noise_params = {\"mean\":0, \"std\":0, 'gamma':0}\n",
        "\n",
        "            else:\n",
        "                train_noise = noise_type\n",
        "                test_noise = noise_type #could be none?\n",
        "                noise_params = {'mean': 0, 'std': std, 'gamma': gamma}\n",
        "            #make datasets\n",
        "            train_ds, val_ds, test_ds_noised, in_channels = make_datasets(train_noise=train_noise,\n",
        "                                                                   test_noise = test_noise,\n",
        "                                                            noise_params=noise_params,\n",
        "                                                            group_order=group_order)\n",
        "            _, _, test_ds_clean, _ = make_datasets(test_noise=None, noise_params=noise_params,\n",
        "                                                            group_order=group_order)\n",
        "            #train model\n",
        "            run_data, best_model = train_full(model_name=model_name,\n",
        "                          train_ds=train_ds,\n",
        "                          val_ds=val_ds,\n",
        "                          in_channels=in_channels,\n",
        "                          hidden_dim=hidden_dim,\n",
        "                          out_channels=out_channels,\n",
        "                          classes=classes,\n",
        "                          num_gconvs=num_gconvs,\n",
        "                          kernel_size=kernel_size,\n",
        "                          group_order=group_order,\n",
        "                          num_epochs=num_epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          learning_rate=learning_rate,\n",
        "                          device=device)\n",
        "            run_data['noise_type'] = noise_type\n",
        "            run_data['std'] = std\n",
        "\n",
        "            run_dir = make_run_dir(model_name, noise_type, noise_params = noise_params, learning_rate = learning_rate)\n",
        "            torch.save(best_model.state_dict(), run_dir)\n",
        "            with open(run_dir / \"run_data.json\", \"w\") as f:\n",
        "                json.dump(run_data, f, default = to_serializable, indent=2)\n",
        "\n",
        "            #test the model\n",
        "            test_noisy_loader = torch.utils.data.DataLoader(test_ds_noised, batch_size=batch_size, shuffle=False, pin_memory =True)\n",
        "            test_clean_loader = torch.utils.data.DataLoader(test_ds_clean, batch_size=batch_size, shuffle=False, pin_memory =True)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            test_loss, test_acc = evaluate(best_model,  device, test_noisy_loader, criterion = criterion)\n",
        "            test_loss_clean, test_acc_clean = evaluate(best_model,  device, test_clean_loader, criterion = criterion)\n",
        "            test = {\n",
        "                \"test_noisy_loss\": test_loss,\n",
        "                \"test__noisy_acc\": test_acc,\n",
        "                \"test_clean_loss\": test_loss_clean,\n",
        "                \"test_clean_acc\": test_acc_clean\n",
        "            }\n",
        "            print(f\"Test Loss: {test_loss}, Test Acc: {test_acc}\")\n",
        "            with open(run_dir / \"test_data.json\", \"w\") as f:\n",
        "                json.dump(test, f, default = to_serializable, indent=2)\n",
        "\n"
      ],
      "id": "78e5ef70973b81ce"
    },
    {
      "metadata": {
        "id": "49af4e72ff275239"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting #\n"
      ],
      "id": "49af4e72ff275239"
    },
    {
      "metadata": {
        "id": "cd9676f8c10d0ad3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#first, plot the performance of all 3 models on the baseline test set, on two sets of axes: one for train, one for validation\n",
        "fig, (train_ax, val_ax) = plt.subplots(1,2)\n",
        "runs = []\n",
        "tests = []\n",
        "for model_name in models:\n",
        "    runpath = Path(f\"./runs/{model_name}_none_std0_gamma0_lr{learning_rate}\")\n",
        "\n",
        "    #load the rundata in from the json\n",
        "    rundatapath = runpath / \"run_data.json\"\n",
        "    testdatapath = runpath / \"test_data.json\"\n",
        "    with rundatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        run_data = json.load(f)\n",
        "    with testdatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)\n",
        "    run_data.update(test_data)\n",
        "    runs.append(run_data)\n",
        "    tests.append(test_data)\n",
        "\n",
        "#make loss plots\n",
        "train_ax = LossPlot(train_ax, runs, labels = [\"model_name\"], title = \"Training Loss\", val = False)\n",
        "val_ax = LossPlot(val_ax, runs, labels = ['model_name'], title = \"Validation Loss\", train = False)\n",
        "fig.tight_layout()\n",
        "fig.suptitle(\"Training and Validation Loss, Baseline (No Noise) Experiments\", y=1.02)\n",
        "fig.savefig(\"baseline_loss_plots.png\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "cd9676f8c10d0ad3"
    },
    {
      "metadata": {
        "id": "69744df26d23b43d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# do the same for each noise type, but plotting both std runs on the same axes\n",
        "#so, each axes object will have 6 lines: 3 models x 2 stds\n",
        "noise_types = ['iso', 'aniso']\n",
        "for noise_type in noise_types:\n",
        "    fig, (train_ax, val_ax) = plt.subplots(1,2)\n",
        "    runs = []\n",
        "    tests = []\n",
        "    for model_name in models:\n",
        "        for std in stds:\n",
        "            runpath = Path(f\"./runs/{model_name}_{noise_type}_std{std}_gamma{gamma}_lr{learning_rate}\")\n",
        "\n",
        "            #load the rundata in from the json\n",
        "            rundatapath = runpath / \"run_data.json\"\n",
        "            testdatapath = runpath / \"test_data.json\"\n",
        "            with rundatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                run_data = json.load(f)\n",
        "            with testdatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                test_data = json.load(f)\n",
        "            run_data.update(test_data)\n",
        "            runs.append(run_data)\n",
        "            tests.append(test_data)\n",
        "\n",
        "    #make loss plots\n",
        "    train_ax = LossPlot(train_ax, runs, labels = [\"model_name\", \"std\"], title = \"Training Loss\", val = False)\n",
        "    val_ax = LossPlot(val_ax, runs, labels = ['model_name', \"std\"], title = \"Validation Loss\", train = False)\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle(f\"Training and Validation Loss, {noise_type.capitalize()} Noise Experiments\", y=1.02)\n",
        "    fig.savefig(f\"{noise_type}_loss_plots.png\")\n"
      ],
      "id": "69744df26d23b43d"
    },
    {
      "metadata": {
        "id": "2eb5507140c8a582"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#fianlly, generate a dataframe for test accuracies across all runs\n",
        "import pandas as pd\n",
        "all_runs = []\n",
        "for model_name in models:\n",
        "    for noise_type in noise_types + ['none']:\n",
        "        if noise_type == 'none':\n",
        "            runpath = Path(f\"./runs/{model_name}_none_std0_gamma0_lr{learning_rate}\")\n",
        "            std = 0\n",
        "        for std in stds:\n",
        "            runpath = Path(f\"./runs/{model_name}_{noise_type}_std{std}_gamma{gamma}_lr{learning_rate}\")\n",
        "            #load the rundata in from the json\n",
        "            rundatapath = runpath / \"run_data.json\"\n",
        "            testdatapath = runpath / \"test_data.json\"\n",
        "            with rundatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                run_data = json.load(f)\n",
        "            with testdatapath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                test_data = json.load(f)\n",
        "            run_data.update(test_data)\n",
        "            all_runs.append(run_data)\n",
        "df = pd.DataFrame(all_runs)\n",
        "df = df[['model_name', 'noise_type', 'std', 'test_acc']] #get only the stuff we want\n",
        "df.to_csv(\"test_accuracies.csv\", index=False)"
      ],
      "id": "2eb5507140c8a582"
    },
    {
      "metadata": {
        "id": "c86c48c3380646b8"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "c86c48c3380646b8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}